{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tqdm as tqdm\n",
    "import shutil\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import os, argparse\n",
    "import cv2, numpy as np\n",
    "# from sklearn.externals import joblib\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imutils import paths\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22c00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU, add, Conv2D, Reshape\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image, text, sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb26d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa_util.vqa_model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee07f15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function vqa_util.vqa_model.build_model(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = text.Tokenizers(filters='')\n",
    "# load from disk\n",
    "with open('./something.pkl', 'rb') as f:\n",
    "    tok = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "with open('./something.pkl', mode='rb') as f:\n",
    "    question_data_train, question_data_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23405645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "with open('./something.pkl', 'rb') as f:\n",
    "    labelencoder = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_matrix(answers, encoder):\n",
    "\t'''\n",
    "\tOne-hot-encodes the answers\n",
    "\n",
    "\tInput:\n",
    "\t\tanswers:\tlist of answer\n",
    "\t\tencoder:\ta scikit-learn LabelEncoder object\n",
    "  \n",
    "\tOutput:\n",
    "\t\tA numpy array of shape (# of answers, # of class)\n",
    "\t'''\n",
    "\ty = encoder.transform(answers) #string to numerical class\n",
    "\tnb_classes = encoder.classes_.shape[0]\n",
    "\tY = utils.to_categorical(y, nb_classes)\n",
    "\treturn Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf551d",
   "metadata": {},
   "source": [
    "### Prepare data matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size= 0.25,random_state=42)\n",
    "\n",
    "for train_index, val_index in sss.split(images_train, answer_train):\n",
    "    TRAIN_INDEX = train_index\n",
    "    VAL_INDEX = val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image data\n",
    "image_list_tr, image_list_vl = np.array(images_train)[TRAIN_INDEX.astype(int)], np.array(images_train)[VAL_INDEX.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question data\n",
    "question_tr, question_vl = question_data_train[TRAIN_INDEX], question_data_train[VAL_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer data\n",
    "answer_matrix = get_answers_matrix(answer_train, labelencoder)\n",
    "answer_tr, answer_vl = answer_matrix[TRAIN_INDEX], answer_matrix[VAL_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb45d5b",
   "metadata": {},
   "source": [
    "## Create tf.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83676434",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 300\n",
    "BUFFER_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the numpy files\n",
    "def map_func(img_name, ques, ans):\n",
    "    img_tensor = np.load('features/' + img_name.decode('utf-8').split('.')[0][-6:] + '.npy')\n",
    "    return img_tensor, ques, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr = tf.data.Dataset.from_tensor_slices((image_list_tr, question_tr, answer_tr))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset_tr = dataset_tr.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset_tr = dataset_tr.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_tr = dataset_tr.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vl = tf.data.Dataset.from_tensor_slices((image_list_vl, question_vl, answer_vl))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset_vl = dataset_vl.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset_vl = dataset_vl.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_vl = dataset_vl.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863cc21d",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80532f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 300\n",
    "BUFFER_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd218ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params 1\n",
    "max_answers = 1000\n",
    "max_seq_len = 22\n",
    "vocab_size  = len(tok.word_index) + 1\n",
    "EPOCHS      = 60\n",
    "\n",
    "dim_d       = 512\n",
    "dim_k       = 256\n",
    "l_rate      = 1e-4\n",
    "d_rate      = 0.5\n",
    "reg_value   = 0.01\n",
    "\n",
    "base_path = './temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd31340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = build_model(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00246b67",
   "metadata": {},
   "source": [
    "Select loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(len(image_list_tr)/BATCH_SIZE))\n",
    "boundaries      = [50*steps_per_epoch]\n",
    "values          = [l_rate, l_rate/10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa72ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reduce the l_rate after 50th epoch (from 1e-4 to 1e-5)\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "optimizer        = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "\n",
    "loss_object      = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ea55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = base_path+\"/training_checkpoints/\"+str(l_rate)+\"_\"+str(dim_k)\n",
    "SAVE_CKPT_FREQ = 5\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_directory, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf637619",
   "metadata": {},
   "source": [
    "Create stateful metrics that can be used to accumulate values during training and logged at any point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "train_score = F1Score(num_classes=max_answers, average='micro', name='train_score')\n",
    "val_score = F1Score(num_classes=max_answers, average='micro', name='val_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eadfcf",
   "metadata": {},
   "source": [
    "Configure the tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_dir = base_path+'/logs/'+str(l_rate)+\"_\"+str(dim_k)+'/train'\n",
    "val_log_dir   = base_path+'/logs/'+str(l_rate)+\"_\"+str(dim_k)+'/validation'\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b6274",
   "metadata": {},
   "source": [
    "Define the training and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf26ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(model, img, ques, ans, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward pass\n",
    "        predictions = model([img, ques], training=True)\n",
    "        loss = loss_object(ans, predictions)\n",
    "\n",
    "    # backward pass\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # record results\n",
    "    train_loss(loss)\n",
    "    train_score(ans, predictions)\n",
    "\n",
    "    # all gradients\n",
    "    grads_ = list(zip(grads, model.trainable_variables))\n",
    "    return grads_\n",
    "\n",
    "def test_step(model, img, ques, ans):\n",
    "    predictions = model([img, ques])\n",
    "    loss = loss_object(ans, predictions)\n",
    "\n",
    "    # record results\n",
    "    val_loss(loss)\n",
    "    val_score(ans, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if manager.latest_checkpoint:\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    START_EPOCH = int(manager.latest_checkpoint.split('-')[-1]) * SAVE_CKPT_FREQ\n",
    "    print(\"Resume training from epoch: {}\".format(START_EPOCH))\n",
    "else:\n",
    "    print(\"Initializing from scratch\")\n",
    "    START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2f45fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'START_EPOCH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e24051092cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'START_EPOCH' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for img, ques, ans in (dataset_tr):\n",
    "        grads = train_step(model, img, ques, ans, optimizer)\n",
    "\n",
    "    # tensorboard  \n",
    "    with train_summary_writer.as_default():\n",
    "        # Create a summary to monitor cost tensor\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        tf.summary.scalar('f1_score', train_score.result(), step=epoch)\n",
    "        # Create summaries to visualize weights\n",
    "        for var in model.trainable_variables:\n",
    "            tf.summary.histogram(var.name, var, step=epoch)\n",
    "        # Summarize all gradients\n",
    "        for grad, var in grads:\n",
    "            tf.summary.histogram(var.name + '/gradient', grad, step=epoch)\n",
    "\n",
    "    for img, ques, ans in (dataset_vl):\n",
    "        test_step(model, img, ques, ans)\n",
    "\n",
    "    # tensorboard\n",
    "    with val_summary_writer.as_default():\n",
    "        # Create a summary to monitor cost tensor\n",
    "        tf.summary.scalar('loss', val_loss.result(), step=epoch)\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        tf.summary.scalar('f1_score', val_score.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, loss: {:.4f}, f1_score: {:.4f}, val loss: {:.4f}, val f1_score: {:.4f}, time: {:.0f} sec'\n",
    "    print (template.format(epoch + 1,\n",
    "                         train_loss.result(), \n",
    "                         train_score.result(),\n",
    "                         val_loss.result(), \n",
    "                         val_score.result(),\n",
    "                         (time.time() - start)))\n",
    "\n",
    "    # Reset metrics every epoch\n",
    "    train_loss.reset_states()\n",
    "    train_score.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_score.reset_states()\n",
    "\n",
    "    # save checkpoint every SAVE_CKPT_FREQ step\n",
    "    ckpt.step.assign_add(1)\n",
    "    if int(ckpt.step) % SAVE_CKPT_FREQ == 0:\n",
    "        manager.save()\n",
    "        print('Saved checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7292018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6316944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical",
   "language": "python",
   "name": "hierarchical"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
